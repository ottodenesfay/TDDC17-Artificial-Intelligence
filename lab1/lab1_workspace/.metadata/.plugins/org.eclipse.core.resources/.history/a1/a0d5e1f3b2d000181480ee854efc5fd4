package tddc17;


import aima.core.environment.liuvacuum.*;
import aima.core.agent.Action;
import aima.core.agent.AgentProgram;
import aima.core.agent.Percept;
import aima.core.agent.impl.*;

import java.util.Random;
import javafx.util.Pair;
import java.util.*;
import java.awt.Point;
import java.util.Collections;

class MyAgentState
{
	public int[][] world = new int[30][30];
	public int initialized = 0;
	final int UNKNOWN 	= 0;
	final int WALL 		= 1;
	final int CLEAR 	= 2;
	final int DIRT		= 3;
	final int HOME		= 4;
	final int ACTION_NONE 			= 0;
	final int ACTION_MOVE_FORWARD 	= 1;
	final int ACTION_TURN_RIGHT 	= 2;
	final int ACTION_TURN_LEFT 		= 3;
	final int ACTION_SUCK	 		= 4;

	public int agent_x_position = 1;
	public int agent_y_position = 1;
	public int agent_last_action = ACTION_NONE;

	public static final int NORTH = 0;
	public static final int EAST = 1;
	public static final int SOUTH = 2;
	public static final int WEST = 3;
	public int agent_direction = EAST;

	Queue<Integer> action_queue = new LinkedList<Integer>();
	Queue<Point> path_to_target = new LinkedList<Point>();

	public Pair<Integer, Integer> node = new Pair<Integer, Integer>(0,0);
	
	public boolean finished_vacuuming = false;
	
	MyAgentState()
	{
		for (int i=0; i < world.length; i++)
			for (int j=0; j < world[i].length ; j++)
				world[i][j] = UNKNOWN;
		world[1][1] = HOME;
		agent_last_action = ACTION_NONE;
	}
	// Based on the last action and the received percept updates the x & y agent position
	public void updatePosition(DynamicPercept p) {
		Boolean bump = (Boolean)p.getAttribute("bump");

		if (agent_last_action==ACTION_MOVE_FORWARD && !bump) {
			switch (agent_direction) {
			case MyAgentState.NORTH:
				agent_y_position--;
				break;
			case MyAgentState.EAST:
				agent_x_position++;
				break;
			case MyAgentState.SOUTH:
				agent_y_position++;
				break;
			case MyAgentState.WEST:
				agent_x_position--;
				break;
			}
	    }
	}

	public void updateWorld(int x_position, int y_position, int info) {
		world[x_position][y_position] = info;
	}

	public void printWorldDebug() {
		for (int i=0; i < world.length; i++) {
			for (int j=0; j < world[i].length ; j++) {
				if (world[j][i]==UNKNOWN)
					System.out.print(" ? ");
				if (world[j][i]==WALL)
					System.out.print(" # ");
				if (world[j][i]==CLEAR)
					System.out.print(" . ");
				if (world[j][i]==DIRT)
					System.out.print(" D ");
				if (world[j][i]==HOME)
					System.out.print(" H ");
			}
			System.out.println("");
		}
	}
}

class MyAgentProgram implements AgentProgram {

	private int initnialRandomActions = 10;
	private Random random_generator = new Random();

	// Here you can define your variables!
	public int iterationCounter = 100000;
	public MyAgentState state = new MyAgentState();

	// moves the Agent to a random start position
	// uses percepts to update the Agent position - only the position, other percepts are ignored
	// returns a random action
	private Action moveToRandomStartPosition(DynamicPercept percept) {
		int action = random_generator.nextInt(6);
		initnialRandomActions--;
		state.updatePosition(percept);
		if(action==0) {
		    state.agent_direction = ((state.agent_direction-1) % 4);
		    if (state.agent_direction<0)
		    	state.agent_direction +=4;
		    state.agent_last_action = state.ACTION_TURN_LEFT;
			return LIUVacuumEnvironment.ACTION_TURN_LEFT;
		} else if (action==1) {
			state.agent_direction = ((state.agent_direction+1) % 4);
		    state.agent_last_action = state.ACTION_TURN_RIGHT;
		    return LIUVacuumEnvironment.ACTION_TURN_RIGHT;
		}
		state.agent_last_action=state.ACTION_MOVE_FORWARD;
		return LIUVacuumEnvironment.ACTION_MOVE_FORWARD;
	}

	public Action moveForward() {
		state.agent_last_action = state.ACTION_MOVE_FORWARD;
		return LIUVacuumEnvironment.ACTION_MOVE_FORWARD;
	}

	public Action turnRight() {
		state.agent_direction = ((state.agent_direction+1) % 4);
		state.agent_last_action = state.ACTION_TURN_RIGHT;
		return LIUVacuumEnvironment.ACTION_TURN_RIGHT;
	}

	public Action turnLeft() {
		state.agent_direction = ((state.agent_direction-1) % 4);
		if (state.agent_direction < 0)
	    	state.agent_direction += 4;
		state.agent_last_action = state.ACTION_TURN_LEFT;
		return LIUVacuumEnvironment.ACTION_TURN_LEFT;
	}

	/* Find the parameter 'goal' (parameter) */
	public void find_node(int goal) {
		Queue<Point> frontier = new LinkedList<Point>();
		Map<Point, Point> nodes_with_parent = new HashMap<Point, Point>();
		Point starting_node = new Point(state.agent_x_position, state.agent_y_position);
		Point next_node = new Point(0,0);
		
		frontier.offer(starting_node);
		/* Set the parent of the current position to null, used when finding a path to an unknown node */
		nodes_with_parent.put(starting_node, null);
		
		while (!frontier.isEmpty()) {
			next_node = frontier.poll();
			/* If the node polled from frontier is unknown, find a path to that node
			by adding the parent of the unknown node, the parent of that parent and so forth
			until the parent of the current node is null (the starting node) */
			if (state.world[(int)next_node.getX()][(int)next_node.getY()] == goal
				&& next_node.getX() >= 0 && next_node.getY() >= 0
				&& next_node.getX() <= 30 && next_node.getY() <= 30) {
				/* Find the path to next_node */
				Vector<Point> tmp_path_to_target = new Vector<Point>();
				tmp_path_to_target.add(next_node);
				while (next_node != null) {
					next_node = nodes_with_parent.get(next_node);
					tmp_path_to_target.add(next_node);
				}
				Collections.reverse(tmp_path_to_target);
				tmp_path_to_target.remove(0);
				tmp_path_to_target.remove(0);
				/* Add each node to path_to_target  */
				for (int y = 0; y < tmp_path_to_target.size(); y++) {
					state.path_to_target.offer(tmp_path_to_target.get(y));
				}
				return;
			}
			/* Generate neighbours, disregard if a wall or already exist in nodes_with_parent */
			for (int x = -1; x < 2; x++) {
				for (int y = -1; y < 2; y++) {
					Point neighbour_node = new Point((int)next_node.getX() + x, (int)next_node.getY() + y);
					if (Math.abs(x) != Math.abs(y) 
						&& state.world[(int)neighbour_node.getX()][(int)neighbour_node.getY()] != state.WALL
						&& !nodes_with_parent.containsKey(neighbour_node) && !frontier.contains(neighbour_node)) {
						nodes_with_parent.put(neighbour_node, next_node);
						frontier.offer(neighbour_node);
					}
				}
			}
				
		}
		/* If the while loop breaks, no valid path to an unknown node was found and the vacuum is finished */
		if (!state.finished_vacuuming) {
			System.out.println("Finished vacuuming, return to HOME");
			find_node(state.HOME);
			state.finished_vacuuming = true;
			System.out.println("-------DONE!-------");
			return;
		}
	}
	
	/* Simulates the vacuum moving from the current pos to the node found in find_nearest_unknown() or go_home().
	Adds each action done to the action_queue */
	public void fill_action_queue() {
		int tmp_x = state.agent_x_position;
		int tmp_y = state.agent_y_position;
		int tmp_direction = state.agent_direction;
		while (state.path_to_target.peek() != null) {
			Point tmp = state.path_to_target.poll();
			
			if (tmp_x < tmp.getX()) {
				if (tmp_direction == state.NORTH) {
					state.action_queue.offer(state.ACTION_TURN_RIGHT);
					tmp_direction = ((tmp_direction+1) % 4);
				}
				else if (tmp_direction == state.SOUTH) {
					state.action_queue.offer(state.ACTION_TURN_LEFT);
					tmp_direction = ((tmp_direction-1) % 4);
					if (tmp_direction < 0)
				    	tmp_direction += 4;
				}
				else if (tmp_direction == state.WEST) {
					state.action_queue.offer(state.ACTION_TURN_RIGHT);
					state.action_queue.offer(state.ACTION_TURN_RIGHT);
					tmp_direction = ((tmp_direction+2) % 4);
				}
				state.action_queue.offer(state.ACTION_MOVE_FORWARD);
				tmp_x++;
			}
			
			else if (tmp_x > tmp.getX())  {
				if (tmp_direction == state.NORTH) {
					state.action_queue.offer(state.ACTION_TURN_LEFT);
					tmp_direction = ((tmp_direction-1) % 4);
					if (tmp_direction < 0)
				    	tmp_direction += 4;
				}
				else if (tmp_direction == state.SOUTH) {
					state.action_queue.offer(state.ACTION_TURN_RIGHT);
					tmp_direction = ((tmp_direction+1) % 4);
				}
				else if (tmp_direction == state.EAST) {
					state.action_queue.offer(state.ACTION_TURN_RIGHT);
					state.action_queue.offer(state.ACTION_TURN_RIGHT);
					tmp_direction = ((tmp_direction+2) % 4);
				}
				state.action_queue.offer(state.ACTION_MOVE_FORWARD);
				tmp_x--;
			}
			
			else {
				if (tmp_y < tmp.getY()) {
					if (tmp_direction == state.NORTH) {
						state.action_queue.offer(state.ACTION_TURN_RIGHT);
						state.action_queue.offer(state.ACTION_TURN_RIGHT);
						tmp_direction = ((tmp_direction+2) % 4);
					}
					else if (tmp_direction == state.WEST) {
						state.action_queue.offer(state.ACTION_TURN_LEFT);
						tmp_direction = ((tmp_direction-1) % 4);
						if (tmp_direction < 0)
					    	tmp_direction += 4;
					}
					else if (tmp_direction == state.EAST) {
						state.action_queue.offer(state.ACTION_TURN_RIGHT);
						tmp_direction = ((tmp_direction+1) % 4);
					}
					state.action_queue.offer(state.ACTION_MOVE_FORWARD);
					tmp_y++;
				}
				else  {
					if (tmp_direction == state.WEST) {
						state.action_queue.offer(state.ACTION_TURN_RIGHT);
						tmp_direction = ((tmp_direction+1) % 4);
					}
					else if (tmp_direction == state.EAST) {
						state.action_queue.offer(state.ACTION_TURN_LEFT);
						tmp_direction = ((tmp_direction-1) % 4);
						if (tmp_direction < 0)
					    	tmp_direction += 4;
					}
					else if (tmp_direction == state.SOUTH) {
						state.action_queue.offer(state.ACTION_TURN_RIGHT);
						state.action_queue.offer(state.ACTION_TURN_RIGHT);
						tmp_direction = ((tmp_direction+2) % 4);
					}
					state.action_queue.offer(state.ACTION_MOVE_FORWARD);
					tmp_y--;
				}
			}
		}
	}

	@Override
	public Action execute(Percept percept) {

		// DO NOT REMOVE this if condition!!!
    	if (initnialRandomActions>0) {
    		return moveToRandomStartPosition((DynamicPercept) percept);
    	} else if (initnialRandomActions==0) {
    		// process percept for the last step of the initial random actions
    		initnialRandomActions--;
    		state.updatePosition((DynamicPercept) percept);
			System.out.println("Processing percepts after the last execution of moveToRandomStartPosition()");
			state.agent_last_action=state.ACTION_SUCK;
	    	return LIUVacuumEnvironment.ACTION_SUCK;
    	}

    	// This example agent program will update the internal agent state while only moving forward.
    	// START HERE - code below should be modified!

    	System.out.println("x=" + state.agent_x_position);
    	System.out.println("y=" + state.agent_y_position);
    	System.out.println("dir=" + state.agent_direction);
    	
	    iterationCounter--;

	    if (iterationCounter==0)
	    	return NoOpAction.NO_OP;

	    DynamicPercept p = (DynamicPercept) percept;
	    Boolean bump = (Boolean)p.getAttribute("bump");
	    Boolean dirt = (Boolean)p.getAttribute("dirt");
	    Boolean home = (Boolean)p.getAttribute("home");
	    System.out.println("percept: " + p);

	    // State update based on the percept value and the last action
	    state.updatePosition((DynamicPercept)percept);
	    if (bump) {
			switch (state.agent_direction) {
			case MyAgentState.NORTH:
				state.updateWorld(state.agent_x_position,state.agent_y_position-1,state.WALL);
				break;
			case MyAgentState.EAST:
				state.updateWorld(state.agent_x_position+1,state.agent_y_position,state.WALL);
				break;
			case MyAgentState.SOUTH:
				state.updateWorld(state.agent_x_position,state.agent_y_position+1,state.WALL);
				break;
			case MyAgentState.WEST:
				state.updateWorld(state.agent_x_position-1,state.agent_y_position,state.WALL);
				break;
			}
	    }
	    
	    if (dirt) {
	    	state.updateWorld(state.agent_x_position,state.agent_y_position,state.DIRT);
	    	state.agent_last_action=state.ACTION_SUCK;
	    	return LIUVacuumEnvironment.ACTION_SUCK;
	    }
	    else if (home){
			state.agent_last_action = state.ACTION_NONE;
	    }
	    else
	    	state.updateWorld(state.agent_x_position,state.agent_y_position,state.CLEAR);

	    System.out.println("X: " + state.agent_x_position);
	    System.out.println("Y: " + state.agent_y_position);
	    state.printWorldDebug();

	    /* If the unexplored node (found through find_nearest_unknown) is reached */
	    if (state.action_queue.isEmpty()) {
	    	find_node(state.UNKNOWN);
	    	fill_action_queue();
	    }
	    
	    /* If the vacuum finishes exploring when at x=1 and y=1 */
	    if (state.action_queue.isEmpty() && state.finished_vacuuming) {
	    	state.agent_last_action = state.ACTION_NONE;
	    	return NoOpAction.NO_OP;
	    }
	    
	    /* Perform next action in action_queue */
	    int next_action = state.action_queue.poll();
    	if (next_action == state.ACTION_MOVE_FORWARD)
    		return moveForward();
    	else if (next_action == state.ACTION_TURN_LEFT)
    		return turnLeft();
    	else if (next_action == state.ACTION_TURN_RIGHT)
    		return turnRight();
    	else
    		return NoOpAction.NO_OP;
	}
}

public class MyVacuumAgent extends AbstractAgent {
    public MyVacuumAgent() {
    	super(new MyAgentProgram());
	}
}
